---

title: ""
permalink: /publications/
author_profile: true
---

<html>
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Publications</title>
  <style>
      .transparent-button {
          display: inline-block;
          background-color: transparent;
          border: 1px solid #3498db;
          color: #3498db;
          padding: 6px 14px;
          font-size: 12px;
          font-weight: 500;
          cursor: pointer;
          border-radius: 999px;
          text-decoration: none;
          line-height: 1.4;
          transition:
              background-color 0.2s ease,
              color 0.2s ease,
              box-shadow 0.2s ease,
              transform 0.2s ease,
              border-color 0.2s ease;
      }

      .transparent-button:hover {
          background-color: #3498db;
          color: #ffffff;
          border-color: #3498db;
          box-shadow: 0 4px 10px rgba(52, 152, 219, 0.4);
          transform: translateY(-1px);
      }

      .section-item {
          border: 1px solid #e0e0e0;
          border-radius: 8px;
          padding: 16px 20px;
          margin-bottom: 12px;
          background-color: #fafafa;
          transition: box-shadow 0.2s ease;
      }

      .section-item:hover {
          box-shadow: 0 2px 8px rgba(0, 0, 0, 0.08);
      }

      .section-title {
          margin-top: 24px;
          margin-bottom: 16px;
          padding-bottom: 8px;
          border-bottom: 2px solid #e0e0e0;
      }
  </style>

</head>
<p class="justified-text"> Papers are roughly grouped by topics and sorted based on date. 
The full list of articles is available on my <a href="https://scholar.google.com/citations?user=VIEERisAAAAJ&hl=en">Google Scholar</a>.</p>

<h2 class="section-title">Vision-Language Adapters</h2>

<div class="section-item">
    <strong>R-MMA: Enhancing Vision-Language Models with Recurrent Adapters for Few-Shot and Cross-Domain Generalization</strong><br>
    <em><a href="https://wacv.thecvf.com/">WACV'26</a></em><br>
   Md Fahim*, Fariha Tanjim Shifat*, Fabiha Haider*, Deeparghya Dutta Barua, Md Sakib Ul Rahman Sourove, <strong>Md Farhan Ishmam</strong>, Farhad Alam Bhuiyan<br>
    <a class="transparent-button" href="https://drive.google.com/file/d/1JzHbUd6pui7MxpM32KL3tGhGXQPz5qkf/view" target="_blank" rel="noopener">Paper</a>
    <!-- <a class="transparent-button" href="https://github.com/farhanishmam/BanglaTLit" target="_blank" rel="noopener">Code</a> -->
    <!-- <a class="transparent-button" href="https://huggingface.co/datasets/aplycaebous/BanglaTLit" target="_blank" rel="noopener">Dataset</a> -->
</div>


<h2 class="section-title">Visual Question Answering (VQA)</h2>

<div class="section-item">
    <strong>Enhancing Vision Language Corruption Robustness using Cross Distribution & Prompted Denoisers</strong><br>
    <em><a href="https://wacv.thecvf.com/">WACV'26</a></em><br>
    Sameer Shafayet Latif*, Sadab Shiper*, K. M. Rahiduzzaman Kiran*, <strong>Md Farhan Ishmam*</strong>, Md Azam Hossain, Abu Raihan Mostofa Kamal, Md Hamjajul Ashmafee<br>
    <a class="transparent-button" href="https://github.com/farhanishmam/VLMDenoising/blob/main/paper/Enhancing%20VL%20Corruption%20Robustness%20with%20Cross%20Distribution%20and%20Prompted%20Denoisers.pdf" target="_blank" rel="noopener">Paper</a>
    <a class="transparent-button" href="https://github.com/farhanishmam/VLMDenoising" target="_blank" rel="noopener">Code</a>
    <!-- <button class="transparent-button" onclick="window.location.href='https://openaccess.thecvf.com/content/WACV2025/supplemental/Ishmam_Visual_Robustness_Benchmark_WACV_2025_supplemental.pdf'"><a href="https://openaccess.thecvf.com/content/WACV2025/supplemental/Ishmam_Visual_Robustness_Benchmark_WACV_2025_supplemental.pdf">Supp</a></button>   -->
  <!-- <button class="transparent-button" onclick="window.location.href='https://github.com/ishmamt/VQA-Visual-Robustness-Benchmark'"><a href="https://github.com/ishmamt/VQA-Visual-Robustness-Benchmark">Code</a></button> -->
   <!-- <button class="transparent-button" onclick="window.location.href='https://drive.google.com/file/d/1uT4KG9YW7o_VTsYtsaVcf4iAVyktl1e0/view'"><a href="https://drive.google.com/file/d/1uT4KG9YW7o_VTsYtsaVcf4iAVyktl1e0/view">Poster</a></button> -->
  <!-- <button class="transparent-button" onclick="window.location.href='https://drive.google.com/file/d/17uBXQkbWralJ1le3nU0p1HUJ-nSjRrYf/view'"><a href="https://drive.google.com/file/d/17uBXQkbWralJ1le3nU0p1HUJ-nSjRrYf/view">Slides</a></button> -->
</div>

<div class="section-item">
    <strong>BanglaProtha: Evaluating Vision Language Models in Underrepresented Long-tail Cultural Contexts</strong><br>
    <em><a href="https://wacv.thecvf.com/">WACV'26</a></em><br>
    Md Fahim*, Md Sakib Ul Rahman*, Akm Moshiur Rahman*, ><strong>Md Farhan Ishmam*</strong>, Md Tasmim Rahman, Fariha Tanjim Shifat, Fabiha Haider, Md Farhad Alam Bhuiyan<br>
    <a class="transparent-button" href="https://drive.google.com/file/d/1btEw-N8Nvyf8YBwrmRAQ9WQMdah8yUgO/view" target="_blank" rel="noopener">Paper</a>
    <a class="transparent-button" href="https://github.com/farhanishmam/BanglaProtha" target="_blank" rel="noopener">Code</a>
   <!-- <button class="transparent-button" onclick="window.location.href='https://drive.google.com/file/d/1uT4KG9YW7o_VTsYtsaVcf4iAVyktl1e0/view'"><a href="https://drive.google.com/file/d/1uT4KG9YW7o_VTsYtsaVcf4iAVyktl1e0/view">Poster</a></button> -->
  <!-- <button class="transparent-button" onclick="window.location.href='https://drive.google.com/file/d/17uBXQkbWralJ1le3nU0p1HUJ-nSjRrYf/view'"><a href="https://drive.google.com/file/d/17uBXQkbWralJ1le3nU0p1HUJ-nSjRrYf/view">Slides</a></button> -->
</div>

<div class="section-item">
    <strong>Visual Robustness Benchmark for Visual Question Answering (VQA)</strong><br>
    <em><a href="https://wacv2025.thecvf.com/">WACV'25</a></em><br>
    <strong>Md Farhan Ishmam*</strong>, Ishmam Tashdeed*, Talukder Asir Saadat*, Md Hamjajul Ashmafee, Abu Raihan Mostofa Kamal, Md Azam Hossain<br>
    <a class="transparent-button" href="https://openaccess.thecvf.com/content/WACV2025/papers/Ishmam_Visual_Robustness_Benchmark_for_Visual_Question_Answering_VQA_WACV_2025_paper.pdf" target="_blank" rel="noopener">Paper</a>
    <a class="transparent-button" href="https://openaccess.thecvf.com/content/WACV2025/supplemental/Ishmam_Visual_Robustness_Benchmark_WACV_2025_supplemental.pdf" target="_blank" rel="noopener">Supp</a>
    <a class="transparent-button" href="https://github.com/ishmamt/VQA-Visual-Robustness-Benchmark" target="_blank" rel="noopener">Code</a>
    <a class="transparent-button" href="https://drive.google.com/file/d/1uT4KG9YW7o_VTsYtsaVcf4iAVyktl1e0/view" target="_blank" rel="noopener">Poster</a>
    <a class="transparent-button" href="https://drive.google.com/file/d/17uBXQkbWralJ1le3nU0p1HUJ-nSjRrYf/view" target="_blank" rel="noopener">Slides</a>
    <a class="transparent-button" href="https://drive.google.com/file/d/1TZFlTdcHcKIKArEVZmUfWKVzNZRNxZh8/view" target="_blank" rel="noopener">Video</a>
</div>

<div class="section-item">
    <strong>ChitroJera: A Regionally Relevant Visual Question Answering Dataset for Bangla</strong><br>
    <em><a href="https://ecmlpkdd.org/2025/">ECML-PKDD'25</a></em><br>
    Deeparghya Dutta Barua*, Md Sakib Ul Rahman Sourove*, <strong>Md Farhan Ishmam*</strong>, Fabiha Haider, Fariha Tanjim Shifat, Md Fahim, Farhad Alam Bhuiyan<br>
    <a class="transparent-button" href="https://arxiv.org/abs/2410.14991" target="_blank" rel="noopener">ArXiv</a>
    <a class="transparent-button" href="https://github.com/farhanishmam/ChitroJera" target="_blank" rel="noopener">Code</a>
</div>

<div class="section-item">
    <strong>From Image to Language: A Critical Analysis of Visual Question Answering (VQA) Approaches, Challenges, and Opportunities</strong><br>
    <em><a href="https://www.sciencedirect.com/journal/information-fusion">Information Fusion Journal</a></em> | <em>2024</em><br>
    <strong>Md Farhan Ishmam</strong>, Md Sakib Hossain Shovon, Muhammad Firoz Mridha, Nilanjan Dey<br>
    <a class="transparent-button" href="https://www.sciencedirect.com/science/article/abs/pii/S1566253524000484" target="_blank" rel="noopener">Paper</a>
    <a class="transparent-button" href="https://arxiv.org/abs/2311.00308" target="_blank" rel="noopener">ArXiv</a>
</div>

<h2 class="section-title">Transliteration and Code-Mixing</h2>

<div class="section-item">
    <strong>BanglaTLit: A Benchmark Dataset for Back-Transliteration of Romanized Bangla</strong><br>
    <em><a href="https://2024.emnlp.org/">EMNLP'24 Findings</a></em><br>
   Md Fahim*, Fariha Tanjim Shifat*, Fabiha Haider*, Deeparghya Dutta Barua, Md Sakib Ul Rahman Sourove, <strong>Md Farhan Ishmam</strong>, Farhad Alam Bhuiyan<br>
    <a class="transparent-button" href="https://aclanthology.org/2024.findings-emnlp.859.pdf" target="_blank" rel="noopener">Paper</a>
    <a class="transparent-button" href="https://github.com/farhanishmam/BanglaTLit" target="_blank" rel="noopener">Code</a>
    <a class="transparent-button" href="https://huggingface.co/datasets/aplycaebous/BanglaTLit" target="_blank" rel="noopener">Dataset</a>
    <a class="transparent-button" href="https://drive.google.com/file/d/1o3yzO-N7J2nj04b6pfGSv0WlCcQODW2S/view" target="_blank" rel="noopener">Poster</a>
    <a class="transparent-button" href="https://drive.google.com/file/d/1Nv-mRwBcE0U3IbDSUjiAy9XEdaXc5x1P/view" target="_blank" rel="noopener">Slides</a>
    <a class="transparent-button" href="https://drive.google.com/file/d/1MUZYADGQQl8OA9Z3BYTBJz96cE8p8C1D/view" target="_blank" rel="noopener">Video</a>
</div>

<div class="section-item">
    <strong>BanTH: A Multi-label Hate Speech Detection Dataset for Transliterated Bangla</strong><br>
    <em><a href="https://2025.naacl.org/">NAACL'25 Findings</a></em><br>
    Fabiha Haider*, Fariha Tanjim Shifat*, <strong>Md Farhan Ishmam*</strong>, Deeparghya Dutta Barua, Md Sakib Ul Rahman Sourove, Md Fahim, Farhad Alam Bhuiyan<br>
    <a class="transparent-button" href="https://arxiv.org/abs/2410.13281" target="_blank" rel="noopener">ArVix</a>
    <a class="transparent-button" href="https://github.com/farhanishmam/BanTH" target="_blank" rel="noopener">Code</a>
</div>

<div class="section-item">
    <strong>BnSentMix: A Diverse Bengali-English Code-Mixed Dataset for Sentiment Analysis</strong><br>
    <em><a href="https://loreslm.github.io/">LoResLM@COLING'25</a></em><br>
    Sadia Alam, <strong>Md Farhan Ishmam</strong>, Navid Hasin Alvee, Md Shahnewaz Siddique, Md Azam Hossain, Abu Raihan Mostofa Kamal<br>
    <a class="transparent-button" href="https://aclanthology.org/2025.loreslm-1.4.pdf" target="_blank" rel="noopener">Paper</a>
    <a class="transparent-button" href="https://github.com/Nishita2000/BnSentMix" target="_blank" rel="noopener">Code</a>
    <a class="transparent-button" href="https://huggingface.co/datasets/aplycaebous/BnSentMix" target="_blank" rel="noopener">Dataset</a>
    <a class="transparent-button" href="https://drive.google.com/file/d/12OeZDEZJUVym39iVnE4BLdGFss9HP9SO/view" target="_blank" rel="noopener">Poster</a>
    <a class="transparent-button" href="https://drive.google.com/file/d/1Hn8a6jutymAhXzBIHhpWnZyf8M5upqLl/view" target="_blank" rel="noopener">Slides</a>
</div>
  
<h2 class="section-title">Harmful Content</h2>
<div class="section-item">
    <strong>Penta NLP at EXIST 2024 Task 1â€“3: Sexism Identification, Source Intention, Sexism Categorization In Tweets</strong><br>
    <em><a href="http://nlp.uned.es/exist2024/">EXIST@CLEF'24</a></em><br>
    Fariha Tanjim Shifat, Fabiha Haider, Md Sakib Ul Rahman Sourove, Deeparghya Dutta Barua, <strong>Md Farhan Ishmam</strong>, Md Fahim, Farhad Alam Bhuiyan<br>
    <a class="transparent-button" href="https://ceur-ws.org/Vol-3740/paper-114.pdf" target="_blank" rel="noopener">Paper</a>
    <a class="transparent-button" href="https://github.com/farhanishmam/Penta-Exist-2024" target="_blank" rel="noopener">Code</a>
</div>

<div class="section-item">
    <strong>Penta ML at EXIST 2024: Tagging Sexism in Online Multimodal Content With Attention-enhanced Modal Context</strong><br>
    <em><a href="http://nlp.uned.es/exist2024/">EXIST@CLEF'24</a></em><br>
    Deeparghya Dutta Barua, Md Sakib Ul Rahman Sourove, Fabiha Haider, Fariha Tanjim Shifat, <strong>Md Farhan Ishmam</strong>, Md Fahim, Farhad Alam Bhuiyan<br>
    <a class="transparent-button" href="https://ceur-ws.org/Vol-3740/paper-90.pdf" target="_blank" rel="noopener">Paper</a>
    <a class="transparent-button" href="https://github.com/farhanishmam/Penta-Exist-2024" target="_blank" rel="noopener">Code</a>
</div>

<h2 class="section-title">General Text Classification</h2>
<div class="section-item">
    <strong>FourierKAN outperforms MLP on Text Classification Head Fine-tuning</strong><br>
    <em><a href=" https://sites.google.com/view/neurips2024-ftw/home">FITML@NeurIPS'24</a></em><br>
    Abdullah Al Imran*, <strong>Md Farhan Ishmam*</strong><br>
    <a class="transparent-button" href="https://openreview.net/forum?id=xqNDuVxThU" target="_blank" rel="noopener">Paper</a>
    <a class="transparent-button" href="https://github.com/abdalimran/FR-KAN-Text-Classification" target="_blank" rel="noopener">Code</a>
</div>


  
</body>
</html>
